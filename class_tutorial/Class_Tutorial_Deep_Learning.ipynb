{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1be34c-ee22-4879-b3c9-8426b1d398a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lab: Deep Learning\n",
    "\n",
    "## In this version of the Ch10 lab, we  use the `luz` package, which interfaces to the\n",
    "## `torch` package which in turn links to efficient\n",
    "## `C++` code in the LibTorch library.\n",
    "\n",
    "## This version of the lab was produced by Daniel Falbel and Sigrid\n",
    "## Keydana, both data scientists at Rstudio where these packages were\n",
    "## produced.\n",
    "\n",
    "## An advantage over our original `keras` implementation is that this\n",
    "## version does not require a separate `python` installation.\n",
    "\n",
    "##########################################\n",
    "## Single Layer Network on Hitters Data ##\n",
    "##########################################\n",
    "\n",
    "## Load various packages\n",
    "library(ISLR2)\n",
    "library(glmnet)\n",
    "library(torch)\n",
    "library(luz) # high-level interface for torch\n",
    "library(torchvision) # for datasets and image transformation\n",
    "library(torchdatasets) # for datasets we are going to use\n",
    "library(zeallot)\n",
    "library(ggplot2)\n",
    "library(grf)\n",
    "\n",
    "## Loading the dataset\n",
    "## We use the example data with baseball player salaries from lecture 4\n",
    "Gitters <- na.omit(Hitters)\n",
    "n <- nrow(Gitters)\n",
    "print(paste(\"Number of observations:\", n))\n",
    "\n",
    "## Define tes sample\n",
    "set.seed(13)\n",
    "ntest <- trunc(n / 3)\n",
    "testid <- sample(1:n, ntest)\n",
    "\n",
    "\n",
    "#######################\n",
    "## Linear Regression ##\n",
    "#######################\n",
    "lfit <- lm(Salary ~ ., data = Gitters[-testid, ])\n",
    "summary(lfit)\n",
    "lpred <- predict(lfit, Gitters[testid, ])\n",
    "print(paste(\"MAE:\", mean(abs(Gitters$Salary[testid] - lpred))))\n",
    "\n",
    "## Dafine y and x as matrix\n",
    "x <- scale(model.matrix(Salary ~ . - 1, data = Gitters))\n",
    "print(paste(\"Number of controls:\", ncol(x)))\n",
    "y <- Gitters$Salary\n",
    "\n",
    "############\n",
    "## Lasso ##\n",
    "###########\n",
    "cvfit <- cv.glmnet(x[-testid, ], y[-testid], type.measure = \"mae\")\n",
    "coef(cvfit)\n",
    "cpred <- predict(cvfit, x[testid, ], s = \"lambda.min\")\n",
    "print(paste(\"MAE:\",mean(abs(y[testid] - cpred))))\n",
    "\n",
    "###################\n",
    "## Random Forest ##\n",
    "###################\n",
    "forest <- regression_forest(x[-testid, ], y[-testid])\n",
    "fpred <- predict(forest, x[testid, ])\n",
    "print(paste(\"MAE:\",mean(abs(y[testid] - fpred$prediction))))\n",
    "\n",
    "####################\n",
    "## Neural Network ##\n",
    "####################\n",
    "\n",
    "torch_manual_seed(13)\n",
    "\n",
    "# single hidden layer\n",
    "# 10 hidden units\n",
    "# ReLU activation function\n",
    "# dropout layer, in which a random 40% of the 10 activations from the \n",
    "  # previous layer are set to zero during each iteration of the stochastic \n",
    "  # gradient descent algorithm\n",
    "# One output\n",
    "# linear output function\n",
    "modnn <- nn_module(\n",
    "  initialize = function(input_size) {\n",
    "    self$hidden <- nn_linear(input_size, 10)\n",
    "    self$activation <- nn_relu()\n",
    "    self$dropout <- nn_dropout(0.4)\n",
    "    self$output <- nn_linear(10, 1)\n",
    "  },\n",
    "  forward = function(x) {\n",
    "    x %>%\n",
    "      self$hidden() %>%\n",
    "      self$activation() %>%\n",
    "      self$dropout() %>%\n",
    "      self$output()\n",
    "  }\n",
    ")\n",
    "\n",
    "# Specify optimisation algorithm\n",
    "# Here mse loss\n",
    "modnn <- modnn %>%\n",
    "  setup(\n",
    "    loss = nn_mse_loss(),\n",
    "    optimizer = optim_rmsprop,\n",
    "    metrics = list(luz_metric_mae())\n",
    "  ) %>%\n",
    "  set_hparams(input_size = ncol(x))\n",
    "\n",
    "# Train the neural network in 1500 iterations\n",
    "fitted <- modnn %>%\n",
    "  fit(\n",
    "    data = list(x[-testid, ], matrix(y[-testid], ncol = 1)),\n",
    "    valid_data = list(x[testid, ], matrix(y[testid], ncol = 1)),\n",
    "    epochs = 1500\n",
    "  )\n",
    "#plot(fitted)\n",
    "\n",
    "\n",
    "npred <- predict(fitted, x[testid, ])\n",
    "mean(abs(y[testid] - npred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
